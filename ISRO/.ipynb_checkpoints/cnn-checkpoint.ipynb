{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = open('Data.pkl','rb')\n",
    "Train_data = pkl.load(Data)['train_data']\n",
    "Test_data = pkl.load(Data)['test_data']\n",
    "Data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 120\n",
    "HEIGHT = Train_data[0][0].size\n",
    "WIDTH = 1\n",
    "NUM_TRAIN_EXMP = len(Train_data)\n",
    "BAND = 1\n",
    "FILTER_SIZE = HEIGHT/9\n",
    "FEATURE_NO = 20\n",
    "POOL_SIZE = 5\n",
    "NEURON_COUNT = 100\n",
    "CLASSES = 10\n",
    "LEARNING_RATE = 0.01\n",
    "INDEX = 0\n",
    "EPOCHS_COMPLETED = 0\n",
    "MAX_STEPS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(BATCH_SIZE,\n",
    "                                                         HEIGHT,WIDTH,BAND))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(BATCH_SIZE))\n",
    "    return inputs_placeholder, labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(data):\n",
    "    global INDEX\n",
    "    start = INDEX\n",
    "    INDEX += BATCH_SIZE\n",
    "    end = INDEX\n",
    "    if INDEX>NUM_TRAIN_EXMP :\n",
    "        global EPOCHS_COMPLETED\n",
    "        EPOCHS_COMPLETED += 1\n",
    "        shuffle(data)\n",
    "        start,INDEX = 0,0\n",
    "        INDEX += BATCH_SIZE\n",
    "        end = INDEX\n",
    "    temp = data[start:end]\n",
    "    inputs = np.array([item[0] for item in temp],dtype='float32')\n",
    "    labels = np.array([item[1] for item in temp],dtype='int32')\n",
    "    inputs = np.reshape(inputs,[-1,HEIGHT,1,1])\n",
    "    #print labels\n",
    "    return inputs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data, images_pl, labels_pl):\n",
    "    inputs, labels = next_batch(data)\n",
    "    feed_dict = {\n",
    "      images_pl: inputs,\n",
    "      labels_pl: labels,\n",
    "    }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(input):\n",
    "    with tf.variable_scope('conv')as scope:\n",
    "        weights = tf.get_variable(name=\"weights\",shape=[FILTER_SIZE,1,1,FEATURE_NO],\n",
    "                                  initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "        biases = tf.get_variable(name=\"biases\",shape=[FEATURE_NO],initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(input,weights,strides=[1,1,1,1],padding='VALID')\n",
    "        h_conv = tf.nn.tanh(conv + biases,name=scope.name)\n",
    "        print h_conv\n",
    "        \n",
    "    pool = tf.nn.max_pool(h_conv,ksize=[1,POOL_SIZE,1,1],strides=[1,POOL_SIZE,1,1],padding='SAME',name='pool')\n",
    "    print pool\n",
    "    \n",
    "    \"\"\"with tf.variable_scope('conv2')as scope:\n",
    "        weights = tf.get_variable(name=\"weights\",shape=[4,1,FEATURE_NO,FEATURE_NO],\n",
    "                                  initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "        biases = tf.get_variable(name=\"biases\",shape=[FEATURE_NO],initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool,weights,strides=[1,1,1,1],padding='VALID')\n",
    "        h_conv2 = tf.nn.tanh(conv2 + biases,name=scope.name)\n",
    "        print h_conv2\n",
    "        \n",
    "    pool2 = tf.nn.max_pool(h_conv2,ksize=[1,4,1,1],strides=[1,4,1,1],padding='SAME',name='pool')\n",
    "    print pool2\"\"\"\n",
    "    \n",
    "    with tf.variable_scope('fully_connected')as scope:\n",
    "        reshape = tf.reshape(pool,[BATCH_SIZE,-1])\n",
    "        inp_height = reshape.get_shape()[1]\n",
    "        weights = tf.get_variable(name=\"weights\",shape=[inp_height,NEURON_COUNT],\n",
    "                                  initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "        biases = tf.get_variable(name=\"biases\",shape=[NEURON_COUNT],initializer=tf.constant_initializer(0.1))\n",
    "        fully_connected = tf.nn.tanh(tf.matmul(reshape,weights)+biases,name=scope.name)\n",
    "        print fully_connected\n",
    "    \n",
    "    with tf.variable_scope('final')as scope:\n",
    "        weights = tf.get_variable(name=\"weights\",shape=[NEURON_COUNT,CLASSES],\n",
    "                                  initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "        biases = tf.get_variable(name=\"biases\",shape=[CLASSES],initializer=tf.constant_initializer(0.1))\n",
    "        final = tf.matmul(fully_connected,weights)+biases\n",
    "        print final\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output,labels):\n",
    "    labels = tf.to_int64(labels)\n",
    "    #print labels\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    output, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    print cross_entropy\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss,learning_rate):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(output, labels):\n",
    "    correct = tf.nn.in_top_k(output, labels, 1)\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32),name='evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data):\n",
    "    true_count = 0  \n",
    "    steps_per_epoch = len(data)//BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data,\n",
    "                                   images_placeholder,\n",
    "                                   labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    precision = float(true_count)/ num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        inputs_placeholder, labels_placeholder = placeholder_inputs(BATCH_SIZE)\n",
    "\n",
    "        logits = inference(inputs_placeholder)\n",
    "\n",
    "        losses = loss(logits,labels_placeholder)\n",
    "\n",
    "        train_op = training(losses,LEARNING_RATE)\n",
    "\n",
    "        eval_correct = evaluation(logits,labels_placeholder)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run(init)\n",
    "        \"\"\"feed_dict = fill_feed_dict(Train_data,inputs_placeholder,labels_placeholder)\n",
    "        for step in xrange(50):\n",
    "            sess.run(train_op,feed_dict=feed_dict)\n",
    "        out,loss_v = sess.run([logits,losses],feed_dict=feed_dict)\n",
    "        print sess.run(tf.nn.softmax(out))\n",
    "        print loss_v\n",
    "        for step in xrange(10):\n",
    "            feed_dict = fill_feed_dict(Train_data,inputs_placeholder,labels_placeholder)\n",
    "            evall = sess.run(eval_correct,feed_dict=feed_dict)\n",
    "            print evall\n",
    "        print INDEX\"\"\"\n",
    "        for step in xrange(MAX_STEPS):\n",
    "            start_time = time.time()\n",
    "            feed_dict = fill_feed_dict(Test_data,inputs_placeholder,labels_placeholder)\n",
    "\n",
    "            loss_value,_ = sess.run([losses,train_op],\n",
    "                                   feed_dict=feed_dict)\n",
    "            #print loss_value\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "             \n",
    "        do_eval(sess,eval_correct,inputs_placeholder,labels_placeholder,Train_data)\n",
    "        #print EPOCHS_COMPLETED\n",
    "        #feed_dict = fill_feed_dict(Train_data,inputs_placeholder,labels_placeholder)\n",
    "        #output = sess.run(logits,feed_dict=feed_dict)\n",
    "        #print sess.run(tf.nn.softmax(output))\n",
    "        #out=sess.run(evaluation,feed_dict=feed_dict)\n",
    "        #print out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv/conv:0\", shape=(120, 197, 1, 20), dtype=float32)\n",
      "Tensor(\"pool:0\", shape=(120, 40, 1, 20), dtype=float32)\n",
      "Tensor(\"fully_connected/fully_connected:0\", shape=(120, 100), dtype=float32)\n",
      "Tensor(\"final/add:0\", shape=(120, 10), dtype=float32)\n",
      "Tensor(\"xentropy:0\", shape=(120,), dtype=float32)\n",
      "Step 0: loss = 2.23 (0.341 sec)\n",
      "Step 100: loss = 2.02 (0.004 sec)\n",
      "Step 200: loss = 2.23 (0.004 sec)\n",
      "Step 300: loss = 2.26 (0.004 sec)\n",
      "Step 400: loss = 2.16 (0.006 sec)\n",
      "Step 500: loss = 2.23 (0.004 sec)\n",
      "Step 600: loss = 2.11 (0.003 sec)\n",
      "Step 700: loss = 2.16 (0.004 sec)\n",
      "Step 800: loss = 2.19 (0.006 sec)\n",
      "Step 900: loss = 2.16 (0.004 sec)\n",
      "Step 1000: loss = 2.16 (0.004 sec)\n",
      "Step 1100: loss = 2.19 (0.004 sec)\n",
      "Step 1200: loss = 2.23 (0.007 sec)\n",
      "Step 1300: loss = 2.15 (0.004 sec)\n",
      "Step 1400: loss = 2.23 (0.003 sec)\n",
      "Step 1500: loss = 2.10 (0.004 sec)\n",
      "Step 1600: loss = 2.17 (0.006 sec)\n",
      "Step 1700: loss = 2.12 (0.004 sec)\n",
      "Step 1800: loss = 2.13 (0.004 sec)\n",
      "Step 1900: loss = 2.23 (0.004 sec)\n",
      "  Num examples: 1920  Num correct: 491  Precision @ 1: 0.2557\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
